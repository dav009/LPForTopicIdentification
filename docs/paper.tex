%\documentclass[4pt,a4paper]{article}
\documentclass[4pt,a4paper,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{algorithm}
\usepackage{textcomp}
\usepackage{fontenc}
\usepackage{tipa}
\usepackage{framed}
\usepackage{multicol}
\usepackage{color}
\usepackage{qtree}
\usepackage{graphicx}
\usepackage{amsmath}

\usepackage{cite}
\usepackage{lastpage}
\usepackage{lmodern}




\usepackage[]{hyperref}
\hypersetup{  
	colorlinks=true,
    urlcolor=cyan           % color of external links
}

\author{David Przybilla\\dav.alejandro@gmail.com, davida@coli.uni-saarland.de\\ \\ Term Paper for Unsupervised Semi-supervised
Learning Seminar\\ Universit\"{a}t des Saarlandes}
\title{Classification of short product Reviews using Semi-Supervised Learning}
\begin{document}
\twocolumn[
	 \begin{@twocolumnfalse}
    \maketitle
   \begin{abstract}
      ...
    \end{abstract}
  \end{@twocolumnfalse}
  ]




\section{Introduction}
Twitter and Tuenti are popular microblogging services in Latinamerica,
as prices of Smartphones became more accessible to people big communities
have grown around this services.\\
These communities are actively commenting everything from political events to products experiences.\\
Therefore it has become of interest for many companies to analyze the microblogging data in order to understand
what their clients want of their products, this led naturally to sentiment Analysis.\\ 
\\
One of the subtasks of Sentiment Analysis is to find the Topic or Aspect of an opinion,
for example given a set of opinions about a mobile phone, it is possible to label each comment
with the topic that it is referring to, this labels could be ``battery", ``design", ``operative system" etc.
This Subtask can be modelled as a classification problem, given a set of fixed possible topics.\\
\\
In the Scenario of text classification Semi-supervised techniques present two advantages:\\
First a semi-supervised method should allow to classify text by annotating only a small portion of data,thus reducing the labor of annotating.\\
Second the unlabeled data is used during training, this is important since the amount of unlabeled data in these tasks is abundant and available, therefore it can be used to provide extra knowledge to the model.\\
\\
In ~\cite{Ando:2005:FLP:1046920.1194905} Zhang and Kubota proposed a semi-supervised approach using linear classifiers for multiple learning tasks.\\
They proposed to create additional classifications tasks (Auxiliary Problems) aside from the Target Problem.\\
The underlying idea is that the auxiliary problems will help finding good predictive structures.\\
One of the constraints in this approach is that auxiliary problems should be  able to automatically generate labelled data from the original unlabeled data.
This method has been used in tasks such as Text Chunking ~\cite{Ando:2005:HSL:1219840.1219841}\\
\\
In contrast ~\cite{Zhu:2005:SLG:1104523} proposes a graph approach called label propagation for
doing semi-supervised learning.\\
This approach maps the data to a graph representation, then labelled instances propagate their labels through the graph, allowing unlabelled data to adopt the label of those instances which are similar.\\
~\cite{Speriosu_twitterpolarity} uses Label Propagation for doing Polarity Classification on tweets.\\
\\
Though Classifying text has been a widely studied topic,the focus has been on long documents,
whereas tweets are at most 144 characters long.\\
The new trends in these social networks have led to research about classification in short texts, the latter has  shown that it arises new challenges, and former approaches are not  effective.\\
One of the reasons is that  user generated comments in the mentioned services tend to be extremely short, leading to sparce feature representations.\\
\\
In respect to short text classification ~\cite{Fan:2010:NMC:1916732.1917677} proposes  to do Feature Extension to deal with data sparcity, in his approach each comment is extended with extra words from an expansion vocabulary.\\
~\cite{Gabrilovich:2006:OBB:1597348.1597395} proposes on the other hand to use encyclopedic knowledge from wikipedia for extending the short comments.\\
As opposed to the above ones,  ~\cite{Sun:2012:STC:2348283.2348511} reduces the word space of the comments to keywords and use information retrieval with a voting scheme to find labels for short comments.\\
\\
This paper describes the setup of an experiment for classifying short-text comments of product reviews extracted from twitter.\\
Two semi-supervised techniques are used, Label Propagation ~\cite{Zhu:2005:SLG:1104523}  and  a variation of Structural learning problem for multitasks ~\cite{Ando:2005:FLP:1046920.1194905}.\\
The next sections describe briefly each of the proposed methods and how they were adapted to the task, then the pre-processing involved. \\
Addionally the results are shown, where both methods are compared among themselves and to a supervised approach.
Finally a short conclusion and future work are discussed.\\
\\
The source code of the implementation is available at github\footnote{\url{https://github.com/dav009/LPForTopicIdentification}},the datasets however are not public, since they are constrained by a permission license.

\section{Datasets}
The Datasets of this experiments were provided by Meridean.\\
Meridean\footnote{\url{http://http://merideangroup.com/}} is a Colombian company which extracts tweets
mentioning latinamerican companies or products.\\
\\
The datasets are separated by product.\\
There are 3 datasets in this experiment:
\begin{itemize}
	\item Sportwear dataset
	\item Mobile-Phone dataset
	\item hygienic-product dataset
\end{itemize}

Each dataset is composed of tuples,each tuple contains : the source of the opinion, the opinion, the polarity and the topic.\\

\begin{table}[h]
\centering
\begin{tabular}{| l |}
\hline
\textbf{Comment:}\\
$\#$ProbandoXperiaArcS Gracias @TalkMex.\\
Las fotos se ven tan nitidas.\\
\hline
\textbf{Translation:}\\
$\#$TestingXperiaArcs Thanks to @TalkMex.\\
Pictures are very sharp.\\
\hline
\textbf{Topic:}\\
Camera\\
\hline
\end{tabular}
\caption{Sample Comment}
\label{tab:sampleTuple}
\end{table}

Each dataset contains approximately 5000 tuples.

\section{Pre-processing}

The Datasets used in this experiment are actual data crawled from twitter and other sources, therefore there is a lot of noise in them.\\
Some of the problems are (but not limited) to:\\
\begin{itemize}
	\item Miss spelled words, from typo errors to lack of accents.
	\item Internet Language, replacing some letters for others whose phonetics are similar i.e: ``\textbf{qu}iero" (I want) for ``\textbf{k}iero", abbreviations and expressions (``Jaja'' ``jiji" ... )
	\item Twitter Jargon such as: ``RT:",``@"..
\end{itemize} 

The pre-processing done was the following:
\begin{enumerate}
	\item Remove Strange characters, such as hearts, and other unicode characters
	\item Remove some of the Twitter Jargon 
	\item Use \href{http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/}{TreeTagger} ~\cite{Schmid94probabilisticpart-of-speech} for getting the part of speech and the stem of each word in each comment
	\item discard those words that are not adjectives, nouns or verbs
	\item replace each word by its stem in lowercase
	\item remove accents from words
\end{enumerate}

When it comes to pre-processing there could be wide number of possible 
tasks that can be done, I try to keep it simple given the time constraints. \\
\\
As a result of this pre-processing each comment is converted into a bag of keywords.\\
This will be later transform into a vector space representation.


%% for future work mention that words like 'camera' that turn out to be good predictor, can be biased by the fact that is misswritten 'camera '  'cmarea'

\section{Label Propagation}

This Semi-supervised learning graph method was proposed by Zhu ~\cite{Zhu:2005:SLG:1104523}.\\
The idea behind Label propagation is similar to K-Neighbors, nevertheless Label propagation make use of the unlabeled data during the training process.\\

This approach maps the data to a graph representation.
In this representation each Arc of the graph will connect two nodes only if the two nodes are similar,
the weights of the arcs are directly proportional to the similarity of the incident nodes.\\

With respect to our classification task,each node in the graph corresponds to the feature vector of a comment.
The weights of the arcs are given by the cosine distance among the vectors.\\
The final representation is a complete graph.\\
\\
In the LP algorithm, the label information of any
node in a graph is propagated to nearby nodes
through weighted arcs until a global stable stage is
achieved ~\cite{Chen:2006:REU:1220175.1220192}.
If a weight arc is high, the label will travel easier through the graph.\\
\\
The LP Algorithm iterates until convergence,
at each step the algorithm will push the labels
of the labeled data points through the arcs of high weights.The underlying assumption is that instances among a class will have high weighted arcs
connecting them.\\
At each iteration the labels of the labelled data are clamped.\\

This propagation is done by measuring a transition matrix $T$.
The transition matrix states the probability of propagating the label of an instance,
and it is measured from the weights of the arcs of the graph.\\
\\
Let $T_{ij}$ be the probability of propagating the label of instance $i$ to instance $j$, then it can be defined as:\\
\begin{align}
	T_{ij}=P(i \rightarrow j)=\frac{w_{ij}}{\sum_{k=1}^{n} w_{kj}}
\end{align}

\subsection{Algorithm}
Let $Y_{ij}$ be the soft probability of labeling instance $i$ with label $j$.\\
\\
Given our semi-supervised task, then $Y$ can be divided into:\\
\\
$Y_{U}$:the soft labels for the unlabelled data\\
$Y_{L}$:the soft labels for the labelled data\\
\\
This means that $Y_{L}$ is given at the beginning of the problem
and the target is to find good values for $Y_{U}$.\\
\\
\textbf{Step 1 - Init}: In this step, the Labels for $Y_{L}$ are clamped, and the labels for $Y_{U}$ are randomly initialized.\\
The proof that the initialization values of $Y_{U}$ are not transcendental can be found at ~\cite{Zhu:2005:SLG:1104523}.\\
\\
\textbf{Step 2 - Propagation}: In this step, the Labels of neighbouring nodes are pushed.\\
Let $t$ be the current iteration, then $Y^{t+1}$ is defined as:
\begin{align}
Y^{t+1} = T Y^{t}
\end{align}
\\
\textbf{Step 3 - Clamp Labelled Data}: In this step, the Labels for $Y_{L}$ are clamped.\\
\\
\textbf{Step 4 - Repeat}: Repeat from step 2 until convergence.\\
\\
\textbf{Step 5 - Labeling Approach}: Once there is convergence, a label has to be chosen for each node in the graph, there is more than one approach.\\
The simplest approach would be to pick the label with the highest probability. \\
In ~\cite{Zhu:2005:SLG:1104523} there is an empirical analysis on more sophisticated approaches, there it is shown how this criteria can affect the performance, nevertheless for this experiment the simple approach was used.\\
\\
More sophisticated versions of this approach have been presented and assessed, in ~\cite{Talukdar:2010:EGS:1858681.1858830} Talukdar and Pereira showed an empirical comparison among different variations of the LP algorithm, a modified absortion algorithm is presented where seed labels are not clamped,and it is shown to have better performance in the given tasks.
Nevertheless within this paper we call LP to the algorithm proposed by Zhu ~\cite{Zhu:2005:SLG:1104523}.\\
\\
To summarize, in the experiment we convert our data into a complete graph,
each node is the feature vector of a comment, and the weight of each arc
corresponds to the similarity of the comments being incident to the arc.\\
This Graph is  fed to the Label Propagation Algorithm proposed by Zhu.\\



 
\section{Structure Learning}
A Semi-supervised learning method proposed in ~\cite{Ando:2005:FLP:1046920.1194905}.\\
This paper proposes a way to learn multiple classification tasks.In this approach the original Classification Task(Target Problem) is extended by creating a set of auxiliary classification tasks.\\
The Auxiliary Classification tasks are used to improve performance on the target task,
the underlying assumption is that by solving the auxiliary problems one should be able to find good predictive structures.\\
The Auxiliary Tasks are trained on the unlabeled data,in consequence one constraint to the creation of auxiliary tasks is that they should be able to automatically generate training data from the target's problem unlabelled data.In ~\cite{Ando:2005:HSL:1219840.1219841} it is argued that having a high number of auxiliary tasks is beneficial for the performance.\\
\\
In this paper's experiment, the target task corresponds to find the label of each comment.\\
The auxiliary problems are defined as:\\
Learn a linear predictor using the Unlabelled data for the $K$ most common words in the unlabeled data and a predictor for the words with highest Pointwise mutual information(PMI) from the Labelled Data.\\
\\
For learning this tasks, first the Words with highest PMI are found using the labelled data, then the words with highest frequency in the unlabelled data are found.\\
Subsequently a linear classifier for predicting each of this words is trained using the unlabelled data.
It is clear that labels for this tasks can be easily generated by masking the words for which the classifiers are being trained from the comments.\\
\\
By solving the auxiliary tasks, the feauture vectors of the training data for the target problem can be extended with the predictions of the auxiliary tasks.\\
This might shown useful in the setup of the experiment, since we are extending the feauture representations from knowledge of unlabelled data.\\
\\
Finally a linear classifier is learnt using the training data for the target problem.\\
With respect to the ASO-SVD algorithm, this paper just explore a naive approach by using auxiliary problems, in this regard the auxiliary classifiers are used to extend the feature vectors.


\section{Feature Generation}

\section{Experiment}
For the experiments, the three datasets mentioned above were used as training and test data.\\
The experiments corresponds to the following:
\begin{itemize}
	\item Single task Supervised Learning using SVM ($SVM$)
	\item Single task Semi-supervised Learning using Label Propagation ($LP$)
	\item Single task Semi-supervised Learning using The naive Structural Learning($SL$)
	
\end{itemize}

Each of the experiments were ran with different amounts of training data, specifically 10\%, 30\%, 60\% and 90\%\\
\\
For the Label Propagation  $Junto$ \footnote{ \url{https://github.com/parthatalukdar/junto} } ~\cite{Talukdar:2010:EGS:1858681.1858830} was used. $Junto$ is an open source implementation of the Label Propagation algorithm proposed by Zhu.\\
\\
For $SL$ and $SVM$ the libsvm\footnote{\url{http://www.csie.ntu.edu.tw/~cjlin/libsvm/}} library provided an implementation for training SVM.\\
For all the linear classifiers trained in the experiments a  polynomial Kernel was picked and the value of $gamma$ was 1.2, the reason behind this choice is motivated by the empirical results obtained by ~\cite{Joachims:1998:TCS:645326.649721} in a similar task.




La

\section{Results}

\section{Future Work}
%other LP variations ( MAD for noisy ..)
%multitask
%more feautures for running
%extending the comments with extra knowledge
%better preprocesing

\bibliography{paper}{}
\bibliographystyle{alpha}



\end{document}


